{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(Kaggle) Cactus Aerial Photos Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXNnqIY4n5rD"
      },
      "source": [
        "We will be using the aerial cactus dataset provided by J. Irving Vasquez-Gomez on Kaggle. This dataset is split between two classes, \"cactus\" and \"no cactus\". The datasets provided have already been split into a training and validation set respectively, so we do not necessarily need to split the dataset independently. There are 17,000 images in the training set and 4,000 images in the validation set. We'll be using some of the images from the validation set for testing out the prediction from our models.\n",
        "\n",
        "Please note that you must set your runtime on Colab to GPU as this will speed up model performance. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOMl7DzRL8U6"
      },
      "source": [
        "We want to make sure that we mount our root directory correctly in order to move into the directory where we want to save our data in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0HfDz7XLe80"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1tQBQBmLsxx"
      },
      "source": [
        "from google.colab import drive \n",
        "\n",
        "ROOT = \"/content/drive\"     \n",
        "print(ROOT)                 \n",
        "\n",
        "drive.mount(ROOT)           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbaeZc3-LttO"
      },
      "source": [
        "#%cd /content/drive/My Drive/\n",
        "#make sure to note in which directory you'll be saving the code. \n",
        "#I recommend to save it in a separate directory for the following steps.  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3wkCo6JJvM_"
      },
      "source": [
        "Follow tutorial [here](https://medium.com/@opalkabert/downloading-kaggle-datasets-into-google-colab-fb9654c94235) on how to download kaggle data from Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwa7sfzBJUZe"
      },
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "fDjYoVlbJo87",
        "outputId": "17f9cef1-4080-4700-b31c-ac09088c211d"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1fc36259-8b08-4414-89c6-76a4526c86d1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1fc36259-8b08-4414-89c6-76a4526c86d1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"jofr94\",\"key\":\"f5e4678eef6761c307bb0faaa0b01710\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCjr6aT8Jq0o"
      },
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLHcRgRfJ8ZA"
      },
      "source": [
        "!kaggle datasets download -d irvingvasquez/cactus-aerial-photos\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_v3JZtIJ8Rz"
      },
      "source": [
        "!mkdir dataset/\n",
        "from zipfile import ZipFile \n",
        "with ZipFile('cactus-aerial-photos.zip', 'r') as cactus_images: \n",
        "  cactus_images.extractall('dataset/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4E1jwrS_ca"
      },
      "source": [
        "import os\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbkEna8KpcM6"
      },
      "source": [
        "We want to firstly display the images, so we'll know what we're dealing with from a high level. I'm using the *matplotlib* library as well as python's *os* library to do so. Note that the original size of the images are 32x32, but I'm enlarging them for display purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfyp6Mb9QIiR"
      },
      "source": [
        "def display_images(path):\n",
        "  images = []\n",
        "  labels = []\n",
        "  for img_path in os.listdir('dataset/training_set/training_set/' + path):\n",
        "    images.append(plt.imread('dataset/training_set/training_set/' + path + img_path))\n",
        "    labels.append(img_path)\n",
        "  fig = plt.figure()\n",
        "  index = 0\n",
        "  plt.figure(figsize=(15, 15))\n",
        "  print('There are {} images.'.format(len(images)))\n",
        "  for image, labels in zip(images[:9], labels[:9]):\n",
        "    index += 1\n",
        "    plt.subplot(3, 3, index)\n",
        "    plt.imshow(image)\n",
        "    plt.title(labels)\n",
        "    plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FUspgQbS4-G"
      },
      "source": [
        "display_images('cactus/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su5Cmjb-S-F7"
      },
      "source": [
        "display_images('no_cactus/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDGojtGXf-JP"
      },
      "source": [
        "Here, we'll be setting up the pipeline for data augmentation that will be applied on to the training and validation sets. Keras allows us to do so by using the *ImageDataGenerator* object where we can input the different types of augmentations we can apply to each image, such as, flipping the image, enlarging it, smoothing it, and so on. \n",
        "\n",
        "We'll then use the *flow_from_directory* function to apply it to each of the sets. Please note that for the training set, it is better to have it in a randomized fashion than in its original order. However, for the validation set (and testing set), we must leave it as is as these will be the sets to use for model evaluation and prediction. Because we're dealing with binary classification, we must set the class mode to binary. \n",
        "\n",
        "S/N: About batch sizes, batch sizes are important to experiment with in order to improve on model performance and efficiency. Batch sizes determines how many data points that the model will interpret at a time to see which one will fit into which class. It might be better to start with a smaller batch size, because the model might determine the best classes to fit faster. However, if you find poor model performance with a smaller size, then slowly increase it over time to observe any changes in performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFVE10KYnZ8y"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras \n",
        "\n",
        "dir_train = \"dataset/training_set/training_set/\"\n",
        "dir_valid = \"dataset/validation_set/validation_set/\"\n",
        "\n",
        "target_w, target_h = 32, 32\n",
        "batch_size = 32\n",
        "\n",
        "datagen_train = ImageDataGenerator(rescale=1./255.0,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True, \n",
        "        vertical_flip=True)\n",
        "train_gen = datagen_train.flow_from_directory(\n",
        "    dir_train, \n",
        "    target_size=(target_w, target_h),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    class_mode=\"binary\")\n",
        "validation_gen = datagen_train.flow_from_directory(\n",
        "    dir_valid, \n",
        "    target_size=(target_w, target_h),\n",
        "    batch_size=batch_size, \n",
        "    shuffle=False,\n",
        "    class_mode=\"binary\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZujgv7-phmj"
      },
      "source": [
        "Now, we're going to setup our baseline model. It'll compose of 2 convolutional layers, 1 batch normalization layer, 1 max pooling layer, 2 dropout layers with 30% and 50% probabilities respectively, and 3 fully-connected layers. The last fully-connected layer will be our final output. \n",
        "\n",
        "In addition to these main layers, we'll be also using a \"flattening\" layer in order to reduce the dimensionality of the previous layers. As you may probably pick up, dimensionality reduction is key component to CNNs. It helps speed up the computational process of our model, and it'll allow it to generalize for other data that wasn't seen in the dataset. It's highly important to not overfit the model, otherwise, it'll only predict accurately what it computed from the training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFohJO9wZzQG"
      },
      "source": [
        "base_model = keras.Sequential()\n",
        "base_model.add(keras.layers.Conv2D(16, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=(target_w, target_h, 3)))\n",
        "base_model.add(keras.layers.BatchNormalization())\n",
        "base_model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "base_model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "base_model.add(keras.layers.BatchNormalization())\n",
        "base_model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "base_model.add(keras.layers.Dropout(0.5))\n",
        "base_model.add(keras.layers.Flatten())\n",
        "base_model.add(keras.layers.Dense(64, activation='relu')) \n",
        "base_model.add(keras.layers.Dense(32, activation='relu')) \n",
        "base_model.add(keras.layers.Dropout(0.5)) \n",
        "base_model.add(keras.layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQJYzsbhTwwL"
      },
      "source": [
        "Let's briefly go over some of the parameters that you see here for each label. \n",
        "\n",
        "Prior to our first official layer, we have a *Sequential* object. It's important to note that we'll be encasing our layers into this object as this will allow us to linearly stack everything into one \"data structure\". You can find similarities with this and other linear based data strucutres such as arrays, lists, and stacks! \n",
        "\n",
        "Starting from the convolutional layer, we the first parameter is the filter. The filter determines the number of dimensions for the output after the convolutions have been calculated. To read more about how convolutions are applied to the input image, check [this article](https://connect2compute.wordpress.com/2019/02/19/introduction-to-convolutions-in-deep-learning/) out. The second parameter is the kernel size which is the size of the convolutional window. There is no sure fire way to determine the window's size, but it's best to err on the side of smaller windows to apply multiple convolutions on the image. For the third parameter, we have the activation function which was previously mentioned in the lecture. We're using a [ReLU activation function](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) in order to determine the probability of which aspects of the image the convolution picks up on to place it in certain category. What defers ReLU from other activation functions is that as long as a probability is greater than 0, it'll choose that value, otherwise, it'll be set to 0. Finally, we have the input size, which is 28x28. \n",
        "\n",
        "Recall that the batch normalization layer is a way for us to scale the pixel values in mini-batches. It is a regularization technique which helps us prevent the model from overfitting. \n",
        "\n",
        "Similar in nature, the max pooling layer allows us to aggregate values to reduce the feature dimension space. As you can see, we're determining the pooling size, which is similar to the kernel size in the convolutional layer. Again, there is no sure fire way of determining a proper pooling size, but err on the side of small so we don't end up with too little information. \n",
        "\n",
        "The dropout layer is another dimensionality reduction technique where we can turn off certain hidden neurons in network (before the drop layer is introduced) by certain probability. \n",
        "\n",
        "As you can see, we have a flatten layer introduced here. It allows us to condense all of the layers before that point into one vector. The reason why being that we need to use fully-connected (AKA Dense layers in Keras) layers when setting up our final outputs in a CNN. Fully-connected layers will NOT be able interpret multidimensional feature spaces such as convolutional layers. \n",
        "\n",
        "Next, we have some fully-connected layers. FC layers are still important for a CNN, because it receives all of the inputs from the previous layer. In a regular neural networks, FC layers will compute the dot-product of all of the previous inputs, adding weights to each, before passing a single (or more depending on the activation function) value. \n",
        "\n",
        "Finally, we have our last fully-connected layer. This is an absolute must as this will determine the final output of the CNN. In this case, we're outputting 1, as we're dealing with a binary classification task. It might sound unintuitive as we're dealing with 2 classes, \"cactus\" and \"no_cactus\", but the reason is that we want to output a single class, rather than have 2 possible probabilities. It's either going to be 0 or 1, yes or no, etc. If we were dealing with multiple labels, then we can up the amount of classes need in our final output in order to determine the highest probability. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlLJtsHLaYyE"
      },
      "source": [
        "Now let's print out a summary of the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR_DrhoSnC8Y"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz9vzVcMaefX"
      },
      "source": [
        "As you can see, we're using most of the total parameters here in the CNN, but you can definitely add more regularization techniques in order to decrease the amount. \n",
        "\n",
        "In the next block, we have our optimizer and compiler which passes in said optimizer, as well as a loss function, and an accuracy metric. Before explaining what does an optimizer do, we must first understand the logic behind a loss function. The loss function baasically allows model the minimize the error to find the optimal placement of a datapoint to be placed in a certain class. It is common to use gradient descent under the hood, as it allows us to find the global minimum (which will result in the lowest loss value) by maximizing the steepest rate of change (or descent in this case). To get a more indepth analysis of gradient descent, check out [this article](https://https://analyticsindiamag.com/guide-to-tensorflow-keras-optimizers/). \n",
        "\n",
        "It's important to mention that our loss function deals with binary [cross-entropy](https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e). The cross-entropy part of the equation is the loss function, but this is geared towards binary classification. \n",
        "\n",
        "The optimizer comes in as away to essentially \"optimize\" our gradient descent. It helps us get to our global minimum much faster depending the parameters we set to it. There is no sure fire way, as you can probably pick up by now, of choosing the most optimal parameters, so feel free to experiment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yz9wNwpnQNt"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=0.01)\n",
        "base_model.compile(loss =\"binary_crossentropy\", optimizer=optimizer, \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8D2MDLdd1pH"
      },
      "source": [
        "Here, I am installing a library called livelossplot in order to quickly plot out how well our accuracy scores and loss scores are doing per epoch. Just to add here, you can think of an epoch as an iteration a model makes as it continues to fit onto the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMPR0KG-4ykP"
      },
      "source": [
        "!pip3 install livelossplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXo57GZxeGcj"
      },
      "source": [
        "Here, I have some extra variables that I want to add. These are totally optional, but I would recommend to use them in your practice, as it allows you to get the most optimal version of your model. \n",
        "\n",
        "For the *ReduceLROnPlateau* function, it basically monitors and reduces the learning rate on the optimizer whenever the loss function starts to plateau. Reducing the learning rate can be helpful for getting to the global minimum faster. You can set the lower bound of how far you want the optimizer's learning rate to decrease. \n",
        "\n",
        "As for the *ModelCheckpoint* function, this allows you to save the best weights for further use. This can be helpful for other image classifiers that you'd like to build in the future, but you already have a model that can classify well on similar images. It is set to validation accuracy, so it'll monitor over that metric. \n",
        "\n",
        "As for the *EarlyStopping* function, this allows the model to stop training where it thinks it has found its most optimal model. This is dependent on which metric you'd like to monitor as well as the minimum change it sees as improvement and the number of epochs it'll let performance plateau before stopping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khm9CGy_nXjJ"
      },
      "source": [
        "from livelossplot import PlotLossesKerasTF\n",
        "\n",
        "epochs = 20\n",
        "steps_per_epoch = train_gen.n//train_gen.batch_size #length of the training set / batch size\n",
        "validation_steps = validation_gen.n//validation_gen.batch_size #length of the validation set / batch size \n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
        "                              patience=2, min_lr=0.001, mode='auto')\n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy',\n",
        "                             save_weights_only=True, mode='max', verbose=1)\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto',\n",
        "                           restore_best_weights=True)\n",
        "\n",
        "callbacks = [PlotLossesKerasTF(), checkpoint, reduce_lr, early_stop]\n",
        "\n",
        "history = base_model.fit(\n",
        "    x = train_gen,\n",
        "    steps_per_epoch = steps_per_epoch,\n",
        "    epochs = epochs,\n",
        "    validation_data = validation_gen,\n",
        "    validation_steps = validation_steps,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-OKLppVgLGe"
      },
      "source": [
        "Make sure to monitor and jot down your results from the model fitting process. Did model performed well or did it overfit (high accuracy score in training but low in validation)? \n",
        "\n",
        "Also from these graphs alone, you may start to see the relationship between the accuracy metric and the loss score. They are pretty much an inverse of one another, where as the accuracy metric increases, the loss metric is also decreasing and vice-versa.\n",
        "\n",
        "\n",
        "We can also display a [confusion matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62) to view from a high-level which labels did the model applied correctly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYJzwPgSqSXU"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns \n",
        "\n",
        "def display_matrix(model, validation_gen, validation_steps):\n",
        "  valid_pred = model.predict_generator(validation_gen, validation_steps)\n",
        "  valid_pred = np.argmax(valid_pred, axis=1)\n",
        "  conf_mat = confusion_matrix(validation_gen.classes, valid_pred)\n",
        "\n",
        "\n",
        "  figure = plt.figure(figsize=(8, 8))\n",
        "  sns.heatmap(conf_mat, annot=True,cmap=plt.cm.Blues)\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "display_matrix(base_model, validation_gen, validation_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xIxnAqdqQ7f"
      },
      "source": [
        "Now let's start building out the customized LeNet-5 model from the [paper](https://jivasquez.files.wordpress.com/2019/03/rp_cactus_recognition_elsa-1.pdf), so we can do side by side comparisons with our baseline.\n",
        "\n",
        "Note that I'm defining our data augmentation operations from above once again, because I want to tune the parameters to follow the same format that was in the original paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5H2rIl1ywbB"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras \n",
        "\n",
        "dir_train = \"dataset/training_set/training_set/\"\n",
        "dir_valid = \"dataset/validation_set/validation_set/\"\n",
        "\n",
        "target_w, target_h = 32, 32\n",
        "batch_size = 2500 #using the original batch_size defined by the paper \n",
        "\n",
        "datagen_train = ImageDataGenerator(rescale=1./255.0,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True, \n",
        "        vertical_flip=True)\n",
        "train_gen = datagen_train.flow_from_directory(\n",
        "    dir_train, \n",
        "    target_size=(target_w, target_h),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    class_mode=\"binary\")\n",
        "validation_gen = datagen_train.flow_from_directory(\n",
        "    dir_valid, \n",
        "    target_size=(target_w, target_h),\n",
        "    batch_size=batch_size, \n",
        "    shuffle=False,\n",
        "    class_mode=\"binary\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0GTRQzuIIXs"
      },
      "source": [
        "lenet_model = keras.Sequential()\n",
        "lenet_model.add(keras.layers.Conv2D(6, kernel_size=(5, 5),activation='relu',input_shape=(target_w, target_h, 3)))\n",
        "lenet_model.add(keras.layers.BatchNormalization())\n",
        "lenet_model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "lenet_model.add(keras.layers.Conv2D(16, kernel_size=(5, 5),strides=1,activation='relu'))\n",
        "lenet_model.add(keras.layers.BatchNormalization())\n",
        "lenet_model.add(keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
        "lenet_model.add(keras.layers.Flatten())\n",
        "lenet_model.add(keras.layers.Dense(120, activation='relu'))\n",
        "lenet_model.add(keras.layers.Dense(84, activation='relu'))\n",
        "lenet_model.add(keras.layers.Dropout(0.5))\n",
        "lenet_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "lenet_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjMDkxZQI2M_"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=0.01)\n",
        "lenet_model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdbfY7l2IxE9"
      },
      "source": [
        "epochs = 150 #as noted by paper \n",
        "steps_per_epoch = train_gen.n//train_gen.batch_size #not noted by paper \n",
        "validation_steps = validation_gen.n//validation_gen.batch_size #not noted by paper \n",
        "\n",
        "\n",
        "callbacks = [PlotLossesKerasTF()]\n",
        "\n",
        "history = lenet_model.fit(\n",
        "    x = train_gen,\n",
        "    steps_per_epoch = steps_per_epoch,\n",
        "    epochs = epochs,\n",
        "    validation_data = validation_gen,\n",
        "    validation_steps = validation_steps,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3gMLL3Mgw_-"
      },
      "source": [
        "Make sure to monitor and jot down your results from the model fitting process. Did model performed well as to be expected or did it overfit (high accuracy score in training but low in validation)? \n",
        "\n",
        "We won't be adapting the model to better the performance here (just in case it did underperform), but I encourage you to think of other ways we can reduce the overfitting that is happening here with the LeNet-5 model. \n",
        "\n",
        "Let's also apply the confusion matrix here to see how we're doing from a high-level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WGL_2YGwLFm"
      },
      "source": [
        "display_matrix(lenet_model, validation_gen, validation_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KopAUXzShsB7"
      },
      "source": [
        "Now let's do some actual predictions with our model. We firstly want to to make sure that it's reshaped for the predict function to fit onto it. Then, we'll be choosing two random images from the validation set to see if the model predicts the correct labeling for both. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beti15ahHf-U"
      },
      "source": [
        "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "def processImg(image_path):\n",
        "    img = image.load_img(image_path, target_size=(target_w, target_h))\n",
        "    img = image.img_to_array(img)\n",
        "    img = img.reshape(1, target_w, target_h, 3)\n",
        "    img = preprocess_input(img)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4eMvdzp4lBL"
      },
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "import random\n",
        "\n",
        "cactus_dict = train_gen.class_indices\n",
        "cactus_dict = {y:x for x, y in cactus_dict.items()} #switching key-value pair around\n",
        "print(cactus_dict)\n",
        "cactus_path = 'dataset/validation_set/validation_set/cactus/'\n",
        "no_cactus_path = 'dataset/validation_set/validation_set/no_cactus/'\n",
        "path1 = random.choice(os.listdir(cactus_path))\n",
        "path2 = random.choice(os.listdir(no_cactus_path))\n",
        "\n",
        "img1_prob_base = base_model.predict(processImg(cactus_path+path1))\n",
        "img1_classes_base = cactus_dict[np.argmax(img1_prob_base, axis=1)[0]]\n",
        "img2_prob_base = base_model.predict(processImg(no_cactus_path+path2))\n",
        "img2_classes_base = cactus_dict[np.argmax(img2_prob_base, axis=1)[0]]\n",
        "\n",
        "\n",
        "f = plt.figure()\n",
        "ax1 = f.add_subplot(1, 2, 1)\n",
        "ax1.title.set_text(\"(B Model) Predicted: {}\".format(img1_classes_base))\n",
        "plt.imshow(load_img(cactus_path+path1))\n",
        "plt.xticks([]); plt.yticks([])\n",
        "ax2 = f.add_subplot(1, 2, 2)\n",
        "ax2.title.set_text(\"(B Model) Predicted: {}\".format(img2_classes_base))\n",
        "plt.imshow(load_img(no_cactus_path+path2)) \n",
        "plt.xticks([]); plt.yticks([])\n",
        "plt.show(block=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_-VUL5uH_Ka"
      },
      "source": [
        "img1_prob_lenet = lenet_model.predict(processImg(cactus_path+path1))\n",
        "img1_classes_lenet = cactus_dict[np.argmax(img1_prob_lenet, axis=1)[0]]\n",
        "img2_prob_lenet = lenet_model.predict(processImg(no_cactus_path+path2))\n",
        "print(np.argmax(img2_prob_lenet))\n",
        "img2_classes_lenet = cactus_dict[np.argmax(img2_prob_lenet, axis=1)[0]]\n",
        "\n",
        "f = plt.figure()\n",
        "ax1 = f.add_subplot(1, 2, 1)\n",
        "ax1.title.set_text(\"(L Model) Predicted: {}\".format(img1_classes_lenet))\n",
        "plt.imshow(load_img(cactus_path+path1))\n",
        "plt.xticks([]); plt.yticks([])\n",
        "ax2 = f.add_subplot(1, 2, 2)\n",
        "ax2.title.set_text(\"(L Model) Predicted: {}\".format(img2_classes_lenet))\n",
        "plt.imshow(load_img(no_cactus_path+path2)) \n",
        "plt.xticks([]); plt.yticks([])\n",
        "plt.show(block=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpEFJL0R2EiU"
      },
      "source": [
        "How much better do you think the LeNet-5 model did overall? Or, if our baseline model performed better, how do you think we can modify the original team's customized version so we can achieve better performance? \n",
        "\n",
        "I hope this notebook as well as overall workshop will serve you well in your Machine/Deep Learning journey. Computer Vision is just one of many \"sub-genres\" of Deep Learning, and there are plenty of areas to explore before you start to settle on one you prefer. It's good to be a jack-of-all trades when it comes to Machine Learning but make sure you're highly competent in one area! "
      ]
    }
  ]
}